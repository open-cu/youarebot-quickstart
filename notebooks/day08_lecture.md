# ðŸ’¼ Lecture: Pre-trained Models in Data Scientist and MLE Work

**Format**: 2 hours  
**Tools**: Jupyter Notebooks (10â€“20 pieces)  
**Goal**: to show where and how to use pre-trained models for real tasks, in which cases they work "out of the box", when they require fine-tuning, and where they are useless.  

---

## ðŸ§© Lecture Structure

### ðŸ”· 1. Introduction (10 min)

#### What are pre-trained models?
Pre-trained models are neural networks that have been trained on large, general datasets and then made available for reuse. Think of them as "smart building blocks" that already understand patterns in data.

**Key characteristics:**
- Trained on massive datasets (billions of tokens, millions of images)
- Learn general representations that transfer across tasks
- Available as downloadable checkpoints
- Can be used directly or fine-tuned for specific tasks

**Examples:**
- **BERT**: trained on 3.3B words from Wikipedia + BookCorpus
- **ResNet**: trained on ImageNet (1.2M images, 1000 classes)
- **GPT models**: trained on internet-scale text data

#### Why Preâ€‘trained Models?

**ðŸš€ Speed to Market**
- Traditional approach: 3-6 months to train from scratch
- Pre-trained approach: Days to weeks for deployment

**ðŸ’° Cost Efficiency**
- Shifts cost from unpredictable training to predictable inference
- Training once, use everywhere principle
- Democratizes access to state-of-the-art models

**ðŸ“ˆ Performance Benefits**
- Often outperform models trained from scratch on small datasets
- Benefit from transfer learning
- Built-in robustness from diverse training data

#### Economics vs Quality Analysis

| Scenario                     | Upâ€‘front cost | Inference cost                    | Typical F1 gain | Time to deploy |
| ---------------------------- | ------------- |-----------------------------------| --------------- | -------------- |
| Train BERTâ€‘base from scratch |  â‰ˆ  \$50k GPUs | â‰ˆ  \$0.001 /  req                 |  â€“              | 3-4 months     |
| **GPTâ€‘4o  mini** API          |  \$0          | **\$0.40  /  M  input tokens**    |  +10â€‘15  pp      | Same day       |
| LoRAâ€‘tuned openâ€‘model        |  â‰¤  \$300      | â‰ˆ  \$0.0002 /  req                |  +8â€‘12  pp       | 1-2 weeks      |

**Key insights:**
- APIs are fastest but most expensive long-term
- Fine-tuning offers best cost/performance balance
- Training from scratch only makes sense for very specific domains

#### Where to Find Pre-trained Models

**ðŸ¤— Hugging Face Hub** (Primary source)
- 1.8M+ model checkpoints
- Advanced filtering: by task, license, language, size
- Model cards with usage examples and limitations
- Integration with `transformers`, `diffusers`, `timm`

**Framework-Native Sources**
- **TorchHub**: PyTorch models with `torch.hub.load()`
- **TensorFlow Hub**: TensorFlow models
- **timm**: 1000+ computer vision models
- **sklearn**: Traditional ML models

**Research Sources**
- **Papers with Code**: Latest research with code
- **GitHub repositories**: Direct from authors
- **arXiv**: Cutting-edge research (may need implementation)

#### Usage Modes (The Pre-trained Spectrum)

**ðŸŽ¯ Zero-shot** (No additional training)
- Use model directly out-of-the-box
- Best for: general tasks, proof-of-concepts
- Example: `pipeline("sentiment-analysis", "I love this product!")`

**ðŸ“ Few-shot** (In-context learning)
- Provide examples in the prompt
- Best for: GPT-style models, quick prototyping
- Example: "Translate English to French: Hello -> Bonjour, Goodbye -> ?"

**ðŸ”§ Fine-tuning** (Additional training)
- Train on your specific data
- Best for: domain-specific tasks, production systems
- Methods: full fine-tuning, LoRA, adapter layers

**ðŸ’¡ When to use which approach:**
- Start with zero-shot for validation
- Use few-shot for rapid prototyping
- Fine-tune for production and domain-specific needs

ðŸ“Œ _Demo 1_: Live demonstration of `transformers.pipeline("sentiment-analysis")`
```python
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
result = classifier("I love using pre-trained models!")
# Output: [{'label': 'POSITIVE', 'score': 0.9998}]
```

---

### ðŸ§  2. NLP (30 min)
**Tasks**:
- Classification (sentiment and topic)
- NER (entity extraction)
- Translation
- Summarization
- Zero-shot classification

ðŸ“Œ _Demo 2_: text classification  
ðŸ“Œ _Demo 3_: NER  
ðŸ“Œ _Demo 4_: text translation  
ðŸ“Œ _Demo 5_: zero-shot (`facebook/bart-large-mnli`)  
ðŸ“Œ _Demo 6_: text generation (T5)

ðŸ§© Limitations:
- Context < 512/1024 tokens
- Poor performance on domain-specific vocabulary
- Often require fine-tuning

ðŸ“Œ _Demo 7_: model fine-tuning via PEFT/LoRA

---

### ðŸ§  3. Computer Vision (30 min)

**Tasks**:
- Classification
- Object detection
- Segmentation
- OCR

ðŸ“Œ _Demo 8_: classification with `timm`  
ðŸ“Œ _Demo 9_: object detection (YOLOv8)  
ðŸ“Œ _Demo 10_: segmentation (`DeepLabV3`, `SAM`)  
ðŸ“Œ _Demo 11_: OCR (`TrOCR`, `pytesseract`)

ðŸ§© Limitations:
- Poor performance on medical/satellite/custom images
- Class limitations

ðŸ“Œ _Demo 12_: fine-tuning classifier on custom data

---

### ðŸ§  4. Multimodal Models (15 min)

ðŸ“Œ _Demo 13_: CLIP â€” text and image matching  
ðŸ“Œ _Demo 14_: BLIP-2 â€” image captioning  
ðŸ“Œ _Demo 15_: DINOv2 + FAISS â€” similar image search

ðŸ§© Limitations:
- Interpretability
- Applicability in narrow tasks

---

### ðŸ§  5. Audio and Time Series (15 min)

**Tasks**:
- Audio classification
- ASR (speech recognition)
- Forecasting

ðŸ“Œ _Demo 16_: audio classification (`speechbrain`, `torchaudio`)  
ðŸ“Œ _Demo 17_: Wav2Vec2 (speech-to-text)  
ðŸ“Œ _Demo 18_: time series (`nixtla`, `gluonts`)

ðŸ§© Limitations:
- Noise, accents, frequency
- Few pre-trained models available

---

### ðŸ§  6. Graph ML (5 min)

ðŸ“Œ _Demo 19_: GCN on Cora (with PyG)  
ðŸ§© Limitations: poor scalability, require graph preparation

---

### ðŸ§  7. Code models (5 min)

ðŸ“Œ _Demo 20_: auto-generation of SQL or code using StarCoder / CodeT5  
ðŸ§© Limitations: licensing issues, require result validation

---

### âš ï¸ 8. Where pre-trained models don't work (10 min)

ðŸ›‘ Don't work:
- Tabular ML tasks (like XGBoost)  
- Specific metrics/formulas/business logic

ðŸŸ¡ Partially work:
- Tabular: `TabPFN`, `AutoGluon`  
ðŸ“Œ _Demo 21_: TabPFN on tabular task

---

### ðŸ” 9. Licenses and Legal Restrictions (10 min)

**License types**:
- Apache 2.0 / MIT â€” âœ… commercial use
- CC BY-NC â€” âŒ cannot use in production
- RAIL â€” âš ï¸ usage restrictions (e.g., military)
- OpenRAIL-M / BigScience RAIL

ðŸ“Œ _Demo 22_: `model_info("bigscience/bloom").license`

**How to check**:
- HuggingFace â†’ README / Model Card
- GitHub â†’ LICENSE.md

ðŸ§  Case study: can GPT-2 be used in a corporate assistant?

---

### âœ… 10. Wrap-up (10 min)

- Quick model selection checklist:
  1. Is there a suitable task in the repository?
  2. Does zero-shot work?
  3. Is fine-tuning needed?
  4. Is the license suitable?
- Resources:
  - [https://huggingface.co/models](https://huggingface.co/models)
  - [https://paperswithcode.com](https://paperswithcode.com)
  - [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)
- Homework:
  - choose 1 task
  - find 2â€“3 models
  - test "out of the box"
  - evaluate fine-tuning possibility
  - evaluate license
